<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Speech Processing | Maxime Poli</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Speech Processing" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Master’s degree student at École des Ponts ParisTech" />
<meta property="og:description" content="Master’s degree student at École des Ponts ParisTech" />
<link rel="canonical" href="http://localhost:4000/speech.html" />
<meta property="og:url" content="http://localhost:4000/speech.html" />
<meta property="og:site_name" content="Maxime Poli" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/speech.html","headline":"Speech Processing","description":"Master’s degree student at École des Ponts ParisTech","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/">Maxime Poli</a></h1>
        
        
        
        <p>Master's degree student at École des Ponts ParisTech</p>
        <div style="padding-top:2em">
        <p><a href="vision.html"> Computer Vision </a></p>
        <p> <a href="speech.html"> Speech Processing </a> </p>
        <p> <a href="other.html"> Other </a></p>
        </div>
      </header>
      <section>

      <h1 id="vocal-tract-length-normalization">Vocal Tract Length Normalization</h1>

<p>Extraction of speech features from raw audio is the first step for any speech related application. Some speaker normalization techniques exist to reduce the sensibility of extracted features to speaker, thus focusing on speech content. Among them, vocal tract length normalization (VTLN) aims to compensate for the fact that speakers have vocal tracts of different sizes.</p>

<p>This technique computes a learned factor for each speaker that is used to warp the frequency-axis of the spectra. The pipeline has the following steps : extract MFCC, extract VAD, train UBM model, train VTLN model, extract warp factor from trained VTLN model.</p>

<p>(include audio)</p>

<h2 id="mel-frequency-cepstral-coefficients-mfcc">Mel-Frequency Cepstral Coefficients (MFCC)</h2>

<h3 id="framing">Framing</h3>
<p>The first step is to compute the initial speech features. The audio signal is split into successives frames, each frame length is around 25 milliseconds with a shift of 10 milliseconds. The idea behind this step is that frequencies in a signal change over time, so it would not made sense to do a Fourier transform over the entire signal and instead we do a Fourier transform over each frame.</p>

<h3 id="windowing">Windowing</h3>
<p>To reduce spectral leakage, it is needed to apply a <a href="https://en.wikipedia.org/wiki/Window_function#Spectral_analysis">window function</a> to each frame. A standard window function is the Hamming window \(w\), where for each frame \(0 \leq n \leq N-1\) in the window of size \(N\):</p>

<p>\[w(n) = 0.54 - 0.46 \cos(2 \pi \frac{n}{N-1})\]</p>

<p><img src="assets/vtln/hamming.png" /></p>
<blockquote>
  <p>Hamming window</p>
</blockquote>

<h3 id="spectrogram-filter-banks-and-mfcc">Spectrogram, filter banks and MFCC</h3>
<p>We then compute the <a href="https://en.wikipedia.org/wiki/Spectrogram">spectrogram</a> by doing a short time FFT on each frame.</p>

<p>(include spectrogram)</p>

<p>The final step before being able to compute MFCCs is to apply triangular filters on a <a href="https://en.wikipedia.org/wiki/Mel_scale">mel scale</a> to the power spectrum to extract frequency bands. A mel is a unit of measure based on the human ears perceived frequency. One of the formulas to convert \(f\) hertz into \(m\) mels is:</p>

<p>\[m = \frac{1000}{\log 2} \log (1+ \frac{f}{1000}) \]</p>

<p><img src="assets/vtln/melscale.png" /></p>
<blockquote>
  <p>Triangular filters</p>
</blockquote>

<p>This gives us the <a href="https://en.wikipedia.org/wiki/Filter_bank">filter banks</a></p>

<p>(include filterbanks)</p>

<p>Finally, by taking the log of the spectrum, applying a <a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform">discret cosine transform</a> and by keeping the resulting cepstral coefficients 2-13 and discarding the rest, we get the <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Mel-frequency Cepstral Coefficients</a>.</p>

<p>(include mfcc)</p>

<h3 id="cmvn">CMVN</h3>
<p>Optionally, we can do Cepstral Mean Variance Normalization to those features to minimize noise contamination by transforming the coefficients to have the same statistics (mean 0, variance 1).</p>

<h3 id="voice-activity-detection-vad">Voice Activity Detection (VAD)</h3>
<p>Before training the UBM and the VTLN model, we need to remove frames that do not contain speech using VAD. One of the common and easy way to do this is to use the energy-based VAD, and to remove all frames where the energy is below a given threshold.</p>

<h2 id="train-ubm">Train UBM</h2>

<p>A Universal Background Model - Gaussian Mixtures Model (UBM-GMM) is a speaker-independant model using Gaussian mixtures to represent the distribution of features.</p>

<p>It is trained on a large quantity of speech from a wide population; the parameters of the GMM are computed iteratively via <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-maximization</a> (EM) algorithm.</p>

<h2 id="train-vtln-model">Train VTLN model</h2>


      </section>
      <footer>
          <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
    </footer>
    </div>
  </body>
</html>
